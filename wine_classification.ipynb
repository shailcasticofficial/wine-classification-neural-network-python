{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "8bec90e6-23f2-4aae-8b4a-e9ef999391b7",
    "_uuid": "ae07d14dcb75d8188c49d57cdc54fc36f3397d02",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('W1data.csv')\n",
    "# a = pd.get_dummies(df['Wine'])\n",
    "# # \n",
    "# df = pd.concat([df,a],axis=1)\n",
    "# df1,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('wine.csv')\n",
    "# print(df)\n",
    "a = pd.get_dummies(df['Wine'])\n",
    "df = pd.concat([df,a],axis=1)\n",
    "X = df.drop([1, 2,3,'Wine'], axis = 1)\n",
    "y = df[[1,2,3]].values\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Y_test,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD PROPAGATION\n",
    "# This is the forward propagation function\n",
    "\n",
    "def forward_prop(model,a0):\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the backward propagation function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "90023c7a-8059-4117-98b3-82ceb3e5877a",
    "_uuid": "de660f4b64992b03558fc14ee176479b4b7afc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.9830056008463683\n",
      "Accuracy after iteration 0 : 66.90140845070422 %\n",
      "Loss after iteration 100 : 0.3861134327021541\n",
      "Accuracy after iteration 100 : 88.02816901408451 %\n",
      "Loss after iteration 200 : 0.3125759947512956\n",
      "Accuracy after iteration 200 : 89.43661971830986 %\n",
      "Loss after iteration 300 : 0.2729651494776826\n",
      "Accuracy after iteration 300 : 91.54929577464789 %\n",
      "Loss after iteration 400 : 0.23464984625479302\n",
      "Accuracy after iteration 400 : 92.95774647887323 %\n",
      "Loss after iteration 500 : 0.21589085947401448\n",
      "Accuracy after iteration 500 : 92.95774647887323 %\n",
      "Loss after iteration 600 : 0.20438617088944946\n",
      "Accuracy after iteration 600 : 92.95774647887323 %\n",
      "Loss after iteration 700 : 0.1926462627048859\n",
      "Accuracy after iteration 700 : 92.95774647887323 %\n",
      "Loss after iteration 800 : 0.1811624714439095\n",
      "Accuracy after iteration 800 : 93.66197183098592 %\n",
      "Loss after iteration 900 : 0.17569481651502403\n",
      "Accuracy after iteration 900 : 94.36619718309859 %\n",
      "Loss after iteration 1000 : 0.17098646836244152\n",
      "Accuracy after iteration 1000 : 94.36619718309859 %\n",
      "Loss after iteration 1100 : 0.1635577819177681\n",
      "Accuracy after iteration 1100 : 94.36619718309859 %\n",
      "Loss after iteration 1200 : 0.1472646907868871\n",
      "Accuracy after iteration 1200 : 95.07042253521126 %\n",
      "Loss after iteration 1300 : 0.13823132833586882\n",
      "Accuracy after iteration 1300 : 95.77464788732394 %\n",
      "Loss after iteration 1400 : 0.13204429063126769\n",
      "Accuracy after iteration 1400 : 95.77464788732394 %\n",
      "Loss after iteration 1500 : 0.1261414877879182\n",
      "Accuracy after iteration 1500 : 95.77464788732394 %\n",
      "Loss after iteration 1600 : 0.11885048845808723\n",
      "Accuracy after iteration 1600 : 96.47887323943662 %\n",
      "Loss after iteration 1700 : 0.11548332827377737\n",
      "Accuracy after iteration 1700 : 96.47887323943662 %\n",
      "Loss after iteration 1800 : 0.11316985231926982\n",
      "Accuracy after iteration 1800 : 96.47887323943662 %\n",
      "Loss after iteration 1900 : 0.11129270443437696\n",
      "Accuracy after iteration 1900 : 96.47887323943662 %\n",
      "Loss after iteration 2000 : 0.10966938222889511\n",
      "Accuracy after iteration 2000 : 96.47887323943662 %\n",
      "Loss after iteration 2100 : 0.10825255054557631\n",
      "Accuracy after iteration 2100 : 96.47887323943662 %\n",
      "Loss after iteration 2200 : 0.10700289946446313\n",
      "Accuracy after iteration 2200 : 96.47887323943662 %\n",
      "Loss after iteration 2300 : 0.10587213430326795\n",
      "Accuracy after iteration 2300 : 96.47887323943662 %\n",
      "Loss after iteration 2400 : 0.10482016845340449\n",
      "Accuracy after iteration 2400 : 96.47887323943662 %\n",
      "Loss after iteration 2500 : 0.10381380020287204\n",
      "Accuracy after iteration 2500 : 96.47887323943662 %\n",
      "Loss after iteration 2600 : 0.10283387312652689\n",
      "Accuracy after iteration 2600 : 96.47887323943662 %\n",
      "Loss after iteration 2700 : 0.10190265623414148\n",
      "Accuracy after iteration 2700 : 96.47887323943662 %\n",
      "Loss after iteration 2800 : 0.10104161692599695\n",
      "Accuracy after iteration 2800 : 96.47887323943662 %\n",
      "Loss after iteration 2900 : 0.10022401108814877\n",
      "Accuracy after iteration 2900 : 96.47887323943662 %\n",
      "Loss after iteration 3000 : 0.09941791593278682\n",
      "Accuracy after iteration 3000 : 96.47887323943662 %\n",
      "Loss after iteration 3100 : 0.09862597853593251\n",
      "Accuracy after iteration 3100 : 96.47887323943662 %\n",
      "Loss after iteration 3200 : 0.09790777649636287\n",
      "Accuracy after iteration 3200 : 96.47887323943662 %\n",
      "Loss after iteration 3300 : 0.09731075160360118\n",
      "Accuracy after iteration 3300 : 96.47887323943662 %\n",
      "Loss after iteration 3400 : 0.09681168062027336\n",
      "Accuracy after iteration 3400 : 96.47887323943662 %\n",
      "Loss after iteration 3500 : 0.09637377660436458\n",
      "Accuracy after iteration 3500 : 96.47887323943662 %\n",
      "Loss after iteration 3600 : 0.09597534666315788\n",
      "Accuracy after iteration 3600 : 96.47887323943662 %\n",
      "Loss after iteration 3700 : 0.09560527162363415\n",
      "Accuracy after iteration 3700 : 96.47887323943662 %\n",
      "Loss after iteration 3800 : 0.09525738444470615\n",
      "Accuracy after iteration 3800 : 96.47887323943662 %\n",
      "Loss after iteration 3900 : 0.09492775894288101\n",
      "Accuracy after iteration 3900 : 96.47887323943662 %\n",
      "Loss after iteration 4000 : 0.09461350228118781\n",
      "Accuracy after iteration 4000 : 96.47887323943662 %\n",
      "Loss after iteration 4100 : 0.09431214987302555\n",
      "Accuracy after iteration 4100 : 96.47887323943662 %\n",
      "Loss after iteration 4200 : 0.09402125504790884\n",
      "Accuracy after iteration 4200 : 96.47887323943662 %\n",
      "Loss after iteration 4300 : 0.0937379390578579\n",
      "Accuracy after iteration 4300 : 96.47887323943662 %\n",
      "Loss after iteration 4400 : 0.09345814606178549\n",
      "Accuracy after iteration 4400 : 96.47887323943662 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f9886362b0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define all functions\n",
    "\n",
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "\n",
    "#TRAINING PHASE\n",
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n",
    "\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "losses = []\n",
    "\n",
    "def train(model,X_,y_,learning_rate, epochs, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        \n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model\n",
    "\n",
    "# This is what is returned at the end\n",
    "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X_train,Y_train,learning_rate=0.07,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "cf85a92b-2bc6-4b57-8cd6-53b523add48a",
    "_uuid": "6aca14a72b394b8bc0d83fae7134fd4f8bcbfce2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXm33TWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f9865ea0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  94.44444444444444%\n"
     ]
    }
   ],
   "source": [
    "# print(losses,len(losses))\n",
    "plt.show()\n",
    "test = predict(model,X_test)\n",
    "test = pd.get_dummies(test)\n",
    "Y_test = pd.DataFrame(Y_test)\n",
    "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
